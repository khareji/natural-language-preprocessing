{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# paragraph -->stemming-->tokenize-->stopwrod-->new sentence\n",
    "# paragraph--->lemmatizer-->tokenize-->stopwrod-->new sentence\n",
    "# paragraph--->lower case-->stemming/lemmatizer-->stopword->bow->list of binary no(new sentence)\n",
    "# paragraph--->lower case-->stemming/lemmatizer-->stopword->TF-IDF->list of no(new sentence)\n",
    "# paragraph--->lower case-->tokenize->stemming/lemmatizer-->word2vec---->voc(new dict)-->realationship-->simialrity\n",
    "# paragraph-->lower case-->tokenize->stemming/lemmatizer-->one hot-->dic size-->wordembedding-->padding-->feature dimension--->model->compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paragraph -->stemming-->tokenize-->stopwrod-->new sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk #imprting nltk lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paragragh written by 7 year old girl(cousin-sister)\n",
    "\n",
    "paragraph=\"\"\"My friends name is Samridhi, I called him sami she is my best friend , \n",
    "she is my classmate and she study with me from class 1.\n",
    "Cow give us milk ,In some house people keep cow as pet because cow give them milk and cow are very friendly \n",
    "Our nation is India ,we all live in it and I love my nation very much .\n",
    "In India we speak hindi .Mr.modi is the prime minister\n",
    "Tree give us oxygen and take carbon di oxide.Tree give us shelter and fruit. \n",
    "Name of my school is [T.P.S] Tagore Public School , I study in class 4th ,My hobby is dancing ,playing basket ball and badminton\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My friends name is Samridhi, I called him sami she is my best friend , \n",
      "she is my classmate and she study with me from class 1.\n",
      "Cow give us milk ,In some house people keep cow as pet because cow give them milk and cow are very friendly \n",
      "Our nation is India ,we all live in it and I love my nation very much .\n",
      "In India we speak hindi .Mr.modi is the prime minister\n",
      "Tree give us oxygen and take carbon di oxide.Tree give us shelter and fruit. \n",
      "Name of my school is [T.P.S] Tagore Public School , I study in class 4th ,My hobby is dancing ,playing basket ball and badminton\n",
      "570\n"
     ]
    }
   ],
   "source": [
    "print(paragraph) #printing paragraph\n",
    "print(len(paragraph)) #lengeth of paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer #steggimg lib\n",
    "from nltk.corpus import stopwords # stop word lib that will removoe unwanted word like is am are  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=nltk.sent_tokenize(paragraph) #divding whole paragraph into several sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['My friends name is Samridhi, I called him sami she is my best friend , \\nshe is my classmate and she study with me from class 1.',\n",
       " 'Cow give us milk ,In some house people keep cow as pet because cow give them milk and cow are very friendly \\nOur nation is India ,we all live in it and I love my nation very much .',\n",
       " 'In India we speak hindi .Mr.modi is the prime minister\\nTree give us oxygen and take carbon di oxide.Tree give us shelter and fruit.',\n",
       " 'Name of my school is [T.P.S] Tagore Public School , I study in class 4th ,My hobby is dancing ,playing basket ball and badminton']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(sentence)) # length of our sentence after tokenize\n",
    "sentence # sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer() #object for stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'friend', 'name', 'samridhi', ',', 'I', 'call', 'sami', 'best', 'friend', ',', 'classmat', 'studi', 'class', '1', '.']\n",
      "['cow', 'give', 'us', 'milk', ',', 'In', 'hou', 'peopl', 'keep', 'cow', 'pet', 'cow', 'give', 'milk', 'cow', 'friendli', 'nation', 'india', ',', 'live', 'I', 'love', 'nation', 'much', '.']\n",
      "['In', 'india', 'speak', 'hindi', '.mr.modi', 'prime', 'minist', 'tree', 'give', 'us', 'oxygen', 'take', 'carbon', 'di', 'oxide.tr', 'give', 'us', 'shelter', 'fruit', '.']\n",
      "['name', 'school', '[', 't.p', '.', ']', 'tagor', 'public', 'school', ',', 'I', 'studi', 'class', '4th', ',', 'My', 'hobbi', 'danc', ',', 'play', 'basket', 'ball', 'badminton']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentence)): # creating loop running till the lenght of sentence \n",
    "    words=nltk.word_tokenize(sentence[i]) #tokenize each word of each sentnce\n",
    "    print(words)\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words(\"english\"))] #stemming and stopword \n",
    "    sentence[i]=\" \".join(words) # joing after stemming and stopword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['My friend name samridhi , I call sami best friend , classmat studi class 1 .',\n",
       " 'cow give us milk , In hous peopl keep cow pet cow give milk cow friendli our nation india , live I love nation much .',\n",
       " 'In india speak hindi .mr.modi prime minist tree give us oxygen take carbon di oxide.tre give us shelter fruit .',\n",
       " 'name school [ t.p. ] tagor public school , I studi class 4th , My hobbi danc , play basket ball badminton']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(sentence)) #length of  sentence after stemming\n",
    "sentence #sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paragraph--->lemmatizer-->tokenize-->stopwrod-->new sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer #lemmatizer lib\n",
    "from nltk.corpus import stopwords # stop wqord lib for removing unwanted words\n",
    "paragraph1=\"\"\"My friends name is Samridhi, I called him sami she is my best friend , \n",
    "she is my classmate and she study with me from class 1.\n",
    "Cow give us milk ,In some house people keep cow as pet because cow give them milk and cow are very friendly \n",
    "Our nation is India ,we all live in it and I love my nation very much .\n",
    "In India we speak hindi .Mr.modi is the prime minister\n",
    "Tree give us oxygen and take carbon di oxide.Tree give us shelter and fruit. \n",
    "Name of my school is [T.P.S] Tagore Public School , I study in class 4th ,My hobby is dancing ,playing basket ball and badminton\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1=nltk.sent_tokenize(paragraph1) #takenizing paragraph into sentence\n",
    "lemmatizer=WordNetLemmatizer() # obejct for lemmatizer\n",
    "for i in range(len(sentence1)): #running loop till sentence length\n",
    "    words = nltk.word_tokenize(sentence1[i]) #tokenize each word of each sentnce \n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))] #lemmatizer and stop words\n",
    "    sentence1[i] = ' '.join(words)   #joining after lemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['My friend name Samridhi , I called sami best friend , classmate study class 1 .',\n",
       " 'Cow give u milk , In house people keep cow pet cow give milk cow friendly Our nation India , live I love nation much .',\n",
       " 'In India speak hindi .Mr.modi prime minister Tree give u oxygen take carbon di oxide.Tree give u shelter fruit .',\n",
       " 'Name school [ T.P.S ] Tagore Public School , I study class 4th , My hobby dancing , playing basket ball badminton']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(sentence1)) # new sentence length\n",
    "sentence1 # lemmatizer sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paragraph--->lower case-->stemming/lemmatizer-->stopword->bow->list of binary no(new sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #lib for lowering case\n",
    "from nltk.corpus import stopwords # stop word\n",
    "from nltk.stem.porter import PorterStemmer #stemming\n",
    "from nltk.stem import WordNetLemmatizer # lemmatizer\n",
    "paragraph3=\"\"\"My friends name is Samridhi, I called him sami she is my best friend , \n",
    "she is my classmate and she study with me from class 1.\n",
    "Cow give us milk ,In some house people keep cow as pet because cow give them milk and cow are very friendly \n",
    "Our nation is India ,we all live in it and I love my nation very much .\n",
    "In India we speak hindi .Mr.modi is the prime minister\n",
    "Tree give us oxygen and take carbon di oxide.Tree give us shelter and fruit. \n",
    "Name of my school is [T.P.S] Tagore Public School , I study in class 4th ,My hobby is dancing ,playing basket ball and badminton\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer() #object for stemming\n",
    "wordnet=WordNetLemmatizer() #object for lemmatizer\n",
    "sentences2 = nltk.sent_tokenize(paragraph3) # tokenizeing paragraph into sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['friend name samridhi call sami best friend classmat studi class',\n",
       " 'cow give us milk hous peopl keep cow pet cow give milk cow friendli nation india live love nation much',\n",
       " 'india speak hindi mr modi prime minist tree give us oxygen take carbon di oxid tree give us shelter fruit',\n",
       " 'name school p tagor public school studi class th hobbi danc play basket ball badminton']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [] # creating a corpus list that will store new sentence after lowering case with setmming and stopwords\n",
    "for i in range(len(sentences2)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences2[i]) # lowring case neglecting A----Z/a------z\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "corpus #new sentence \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 1,\n",
       "        2, 0, 0, 0, 1, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 2, 1, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "        0, 2, 2],\n",
       "       [1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 1, 1, 0,\n",
       "        1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer #bow #bag of  word lib\n",
    "cv = CountVectorizer(max_features = 1500) # creating object for bow\n",
    "x = cv.fit_transform(corpus).toarray() # fitting into array\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus)) # length of sentece after lowering case \n",
    "print(len(sentences2)) # length of sentence after tokenizeing the paragrapgh\n",
    "print(len(x)) # length of sentence in binary format after bag of word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paragraph--->lower case-->stemming/lemmatizer-->stopword->TF-IDF->list of no(new sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "paragraph4=\"\"\"My friends name is Samridhi, I called him sami she is my best friend , \n",
    "she is my classmate and she study with me from class 1.\n",
    "Cow give us milk ,In some house people keep cow as pet because cow give them milk and cow are very friendly \n",
    "Our nation is India ,we all live in it and I love my nation very much .\n",
    "In India we speak hindi .Mr.modi is the prime minister\n",
    "Tree give us oxygen and take carbon di oxide.Tree give us shelter and fruit. \n",
    "Name of my school is [T.P.S] Tagore Public School , I study in class 4th ,My hobby is dancing ,playing basket ball and badminton\"\"\"\n",
    "ps = PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences3 = nltk.sent_tokenize(paragraph4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(len(sentences3)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences3[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.30338183, 0.30338183,\n",
       "        0.        , 0.23918972, 0.30338183, 0.        , 0.        ,\n",
       "        0.        , 0.60676367, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.23918972, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.30338183, 0.30338183, 0.        , 0.        ,\n",
       "        0.        , 0.23918972, 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.675083  , 0.        ,\n",
       "        0.        , 0.        , 0.16877075, 0.        , 0.2661216 ,\n",
       "        0.        , 0.        , 0.16877075, 0.1330608 , 0.16877075,\n",
       "        0.16877075, 0.16877075, 0.3375415 , 0.        , 0.        ,\n",
       "        0.        , 0.16877075, 0.        , 0.3375415 , 0.        ,\n",
       "        0.        , 0.16877075, 0.16877075, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.2230057 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.2230057 , 0.        , 0.        , 0.2230057 , 0.35164051,\n",
       "        0.2230057 , 0.        , 0.        , 0.17582025, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.2230057 , 0.2230057 ,\n",
       "        0.2230057 , 0.        , 0.        , 0.        , 0.2230057 ,\n",
       "        0.2230057 , 0.        , 0.        , 0.        , 0.2230057 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.2230057 ,\n",
       "        0.2230057 , 0.        , 0.        , 0.2230057 , 0.        ,\n",
       "        0.44601139],\n",
       "       [0.25937062, 0.25937062, 0.25937062, 0.        , 0.        ,\n",
       "        0.        , 0.20449078, 0.        , 0.        , 0.25937062,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.25937062, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.20449078, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.25937062, 0.        ,\n",
       "        0.25937062, 0.        , 0.        , 0.51874125, 0.        ,\n",
       "        0.        , 0.20449078, 0.25937062, 0.        , 0.25937062,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer #imprting lib for tf-IDF term frequency - inverse documnetry frequency\n",
    "cv = TfidfVectorizer()\n",
    "x = cv.fit_transform(corpus).toarray()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "print(len(sentences3))\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paragraph--->lower case-->tokenize->stemming/lemmatizer-->word2vec---->voc(new dict)-->realationship-->simialrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec # importing lib for word2vec , creating a word to vector with more than 32 dimension\n",
    "import re\n",
    "paragraph5=\"\"\"My friends name is Samridhi, I called him sami she is my best friend , \n",
    "she is my classmate and she study with me from class 1.\n",
    "Cow give us milk ,In some house people keep cow as pet because cow give them milk and cow are very friendly \n",
    "Our nation is India ,we all live in it and I love my nation very much .\n",
    "In India we speak hindi .Mr.modi is the prime minister\n",
    "Tree give us oxygen and take carbon di oxide.Tree give us shelter and fruit. \n",
    "Name of my school is [T.P.S] Tagore Public School , I study in class 4th ,My hobby is dancing ,playing basket ball and badminton\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'\\[[0-9]*\\]',' ',paragraph5)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "text = text.lower()\n",
    "text = re.sub(r'\\d',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my friends name is samridhi, i called him sami she is my best friend , she is my classmate and she study with me from class . cow give us milk ,in some house people keep cow as pet because cow give them milk and cow are very friendly our nation is india ,we all live in it and i love my nation very much . in india we speak hindi .mr.modi is the prime minister tree give us oxygen and take carbon di oxide.tree give us shelter and fruit. name of my school is [t.p.s] tagore public school , i study in class th ,my hobby is dancing ,playing basket ball and badminton\n",
      "565\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'friends', 'name', 'is', 'samridhi', ',', 'i', 'called', 'him', 'sami', 'she', 'is', 'my', 'best', 'friend', ',', 'she', 'is', 'my', 'classmate', 'and', 'she', 'study', 'with', 'me', 'from', 'class', '.'], ['cow', 'give', 'us', 'milk', ',', 'in', 'some', 'house', 'people', 'keep', 'cow', 'as', 'pet', 'because', 'cow', 'give', 'them', 'milk', 'and', 'cow', 'are', 'very', 'friendly', 'our', 'nation', 'is', 'india', ',', 'we', 'all', 'live', 'in', 'it', 'and', 'i', 'love', 'my', 'nation', 'very', 'much', '.'], ['in', 'india', 'we', 'speak', 'hindi', '.mr.modi', 'is', 'the', 'prime', 'minister', 'tree', 'give', 'us', 'oxygen', 'and', 'take', 'carbon', 'di', 'oxide.tree', 'give', 'us', 'shelter', 'and', 'fruit', '.'], ['name', 'of', 'my', 'school', 'is', '[', 't.p.s', ']', 'tagore', 'public', 'school', ',', 'i', 'study', 'in', 'class', 'th', ',', 'my', 'hobby', 'is', 'dancing', ',', 'playing', 'basket', 'ball', 'and', 'badminton']]\n"
     ]
    }
   ],
   "source": [
    "sentences4 = nltk.sent_tokenize(text)\n",
    "sentences4 = [nltk.word_tokenize(sentence) for sentence in sentences4] # takenizing each word of each sentence\n",
    "print(sentences4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['friends', 'name', 'samridhi', ',', 'called', 'sami', 'best', 'friend', ',', 'classmate', 'study', 'class', '.'], ['cow', 'give', 'us', 'milk', ',', 'house', 'people', 'keep', 'cow', 'pet', 'cow', 'give', 'milk', 'cow', 'friendly', 'nation', 'india', ',', 'live', 'love', 'nation', 'much', '.'], ['india', 'speak', 'hindi', '.mr.modi', 'prime', 'minister', 'tree', 'give', 'us', 'oxygen', 'take', 'carbon', 'di', 'oxide.tree', 'give', 'us', 'shelter', 'fruit', '.'], ['name', 'school', '[', 't.p.s', ']', 'tagore', 'public', 'school', ',', 'study', 'class', 'th', ',', 'hobby', 'dancing', ',', 'playing', 'basket', 'ball', 'badminton']]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences4[i] = [word for word in sentences4[i] if word not in stopwords.words('english')] # stop word \n",
    "\n",
    "print(sentences4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1f56a23a518>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(sentences4, min_count=1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'friends': <gensim.models.keyedvectors.Vocab at 0x1f56a23a588>,\n",
       " 'name': <gensim.models.keyedvectors.Vocab at 0x1f56a23a438>,\n",
       " 'samridhi': <gensim.models.keyedvectors.Vocab at 0x1f56a23a7b8>,\n",
       " ',': <gensim.models.keyedvectors.Vocab at 0x1f56a23a278>,\n",
       " 'called': <gensim.models.keyedvectors.Vocab at 0x1f56a23a048>,\n",
       " 'sami': <gensim.models.keyedvectors.Vocab at 0x1f56a23a940>,\n",
       " 'best': <gensim.models.keyedvectors.Vocab at 0x1f56a23a978>,\n",
       " 'friend': <gensim.models.keyedvectors.Vocab at 0x1f56a23a3c8>,\n",
       " 'classmate': <gensim.models.keyedvectors.Vocab at 0x1f56a23a240>,\n",
       " 'study': <gensim.models.keyedvectors.Vocab at 0x1f56a23a4a8>,\n",
       " 'class': <gensim.models.keyedvectors.Vocab at 0x1f56a23a5f8>,\n",
       " '.': <gensim.models.keyedvectors.Vocab at 0x1f56a23a6d8>,\n",
       " 'cow': <gensim.models.keyedvectors.Vocab at 0x1f56a23a828>,\n",
       " 'give': <gensim.models.keyedvectors.Vocab at 0x1f56a23aa20>,\n",
       " 'us': <gensim.models.keyedvectors.Vocab at 0x1f56a23ae80>,\n",
       " 'milk': <gensim.models.keyedvectors.Vocab at 0x1f56a23add8>,\n",
       " 'house': <gensim.models.keyedvectors.Vocab at 0x1f56a23ab70>,\n",
       " 'people': <gensim.models.keyedvectors.Vocab at 0x1f56a2520f0>,\n",
       " 'keep': <gensim.models.keyedvectors.Vocab at 0x1f56a252668>,\n",
       " 'pet': <gensim.models.keyedvectors.Vocab at 0x1f56a252240>,\n",
       " 'friendly': <gensim.models.keyedvectors.Vocab at 0x1f56a2527b8>,\n",
       " 'nation': <gensim.models.keyedvectors.Vocab at 0x1f56a252940>,\n",
       " 'india': <gensim.models.keyedvectors.Vocab at 0x1f56a2528d0>,\n",
       " 'live': <gensim.models.keyedvectors.Vocab at 0x1f56a252a90>,\n",
       " 'love': <gensim.models.keyedvectors.Vocab at 0x1f56a252a20>,\n",
       " 'much': <gensim.models.keyedvectors.Vocab at 0x1f56a252b00>,\n",
       " 'speak': <gensim.models.keyedvectors.Vocab at 0x1f56a252b70>,\n",
       " 'hindi': <gensim.models.keyedvectors.Vocab at 0x1f56a252cc0>,\n",
       " '.mr.modi': <gensim.models.keyedvectors.Vocab at 0x1f56a252208>,\n",
       " 'prime': <gensim.models.keyedvectors.Vocab at 0x1f56a2527f0>,\n",
       " 'minister': <gensim.models.keyedvectors.Vocab at 0x1f56a252780>,\n",
       " 'tree': <gensim.models.keyedvectors.Vocab at 0x1f56a252e10>,\n",
       " 'oxygen': <gensim.models.keyedvectors.Vocab at 0x1f56a252e48>,\n",
       " 'take': <gensim.models.keyedvectors.Vocab at 0x1f56a252d30>,\n",
       " 'carbon': <gensim.models.keyedvectors.Vocab at 0x1f56a252e80>,\n",
       " 'di': <gensim.models.keyedvectors.Vocab at 0x1f56a252ef0>,\n",
       " 'oxide.tree': <gensim.models.keyedvectors.Vocab at 0x1f56a252da0>,\n",
       " 'shelter': <gensim.models.keyedvectors.Vocab at 0x1f56a252c50>,\n",
       " 'fruit': <gensim.models.keyedvectors.Vocab at 0x1f56a252be0>,\n",
       " 'school': <gensim.models.keyedvectors.Vocab at 0x1f56a2529b0>,\n",
       " '[': <gensim.models.keyedvectors.Vocab at 0x1f56a2523c8>,\n",
       " 't.p.s': <gensim.models.keyedvectors.Vocab at 0x1f56a252438>,\n",
       " ']': <gensim.models.keyedvectors.Vocab at 0x1f56a252470>,\n",
       " 'tagore': <gensim.models.keyedvectors.Vocab at 0x1f56a252518>,\n",
       " 'public': <gensim.models.keyedvectors.Vocab at 0x1f56a252400>,\n",
       " 'th': <gensim.models.keyedvectors.Vocab at 0x1f56a2524a8>,\n",
       " 'hobby': <gensim.models.keyedvectors.Vocab at 0x1f56a2524e0>,\n",
       " 'dancing': <gensim.models.keyedvectors.Vocab at 0x1f56a252588>,\n",
       " 'playing': <gensim.models.keyedvectors.Vocab at 0x1f56a2525c0>,\n",
       " 'basket': <gensim.models.keyedvectors.Vocab at 0x1f56a252390>,\n",
       " 'ball': <gensim.models.keyedvectors.Vocab at 0x1f56a2526d8>,\n",
       " 'badminton': <gensim.models.keyedvectors.Vocab at 0x1f56a252828>}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = model.wv.vocab # list of word with dimension after stopwords and stemming or lemmatizer\n",
    "words # words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.4438889e-03,  2.6700944e-03,  3.7446187e-04,  2.2167969e-03,\n",
       "       -7.5005482e-05, -2.0878302e-04,  2.5988652e-03, -2.9697893e-03,\n",
       "        4.2487714e-03,  9.2966889e-04,  3.9649471e-03, -3.0908505e-03,\n",
       "       -4.3678130e-03, -4.9997942e-04,  3.4624399e-03, -2.6078837e-03,\n",
       "       -2.8899221e-03,  3.4559355e-03, -9.7313069e-04,  1.7445059e-03,\n",
       "       -2.0761197e-03, -1.6238077e-03, -3.7576626e-03,  3.4766621e-03,\n",
       "        4.2731818e-03,  1.0990058e-03,  3.9702440e-03, -3.2697143e-03,\n",
       "        4.1981685e-04,  3.3773202e-04,  6.6549791e-04, -7.8657886e-04,\n",
       "       -4.1248109e-03, -1.0547675e-03,  4.7655455e-03, -7.9905835e-04,\n",
       "       -3.6304174e-03,  9.9694438e-04, -3.7383356e-03, -3.2270763e-03,\n",
       "       -5.5389584e-04, -4.2773662e-03, -3.5198217e-03,  3.1688020e-03,\n",
       "       -3.3604382e-03, -2.3213949e-03, -2.3167870e-04,  1.9259312e-03,\n",
       "        1.8111130e-03,  4.3173786e-03,  3.9212783e-03, -2.5551601e-03,\n",
       "        4.5832647e-03,  4.6057417e-03, -1.1098275e-03, -3.0695486e-03,\n",
       "       -8.4147154e-04,  2.9311788e-03,  3.0931595e-03, -4.0767821e-03,\n",
       "       -3.8071680e-03,  4.9591032e-03, -1.6968152e-03,  3.6573359e-03,\n",
       "        1.1352337e-03, -1.3735663e-03,  6.8032992e-04, -3.5989466e-03,\n",
       "        2.1462068e-03,  1.0410567e-03,  1.1279526e-03,  1.8390876e-03,\n",
       "       -3.6583366e-03,  1.6465125e-03,  3.6856490e-03,  3.0416350e-03,\n",
       "        2.3405259e-03, -2.5094941e-03,  2.4765271e-03, -4.7541056e-03,\n",
       "       -1.2212938e-03, -1.5889632e-03,  4.3550212e-04, -4.9069570e-03,\n",
       "       -7.1639114e-04, -4.6116295e-03,  3.9186217e-03, -4.6909191e-03,\n",
       "       -4.1314727e-03, -3.8294427e-04,  1.4480051e-03,  1.4445744e-03,\n",
       "       -1.4093870e-03, -8.3105604e-04,  4.3176902e-03, -4.8461803e-03,\n",
       "       -1.8963643e-03,  3.5082188e-03,  2.7749536e-03,  3.1929151e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = model.wv['cow'] # cheking the word cow with other (relation ship)\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pet', 0.2404009997844696),\n",
       " ('.', 0.1647706925868988),\n",
       " ('take', 0.15857553482055664),\n",
       " ('.mr.modi', 0.1516527682542801),\n",
       " ('sami', 0.1501161754131317),\n",
       " ('keep', 0.09413091838359833),\n",
       " ('hindi', 0.09286441653966904),\n",
       " ('live', 0.08743944764137268),\n",
       " ('speak', 0.08469157665967941),\n",
       " ('india', 0.08377820253372192)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar = model.wv.most_similar('us') #checking similar word i.e word related to us\n",
    "similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paragraph-->lower case-->tokenize->stemming/lemmatizer-->one hot-->dic size-->wordembedding-->padding-->feature dimension--->model->compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot # one hot encoding in keras lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence5=['My friend name samridhi , I call sami best friend , classmat studi class 1 .',\n",
    " 'cow give us milk , In hous peopl keep cow pet cow give milk cow friendli our nation india , live I love nation much .',\n",
    " 'In india speak hindi .mr.modi prime minist tree give us oxygen take carbon di oxide.tre give us shelter fruit .',\n",
    " 'name school [ t.p. ] tagor public school , I studi class 4th , My hobbi danc , play basket ball badminton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My friend name samridhi , I call sami best friend , classmat studi class 1 .',\n",
       " 'cow give us milk , In hous peopl keep cow pet cow give milk cow friendli our nation india , live I love nation much .',\n",
       " 'In india speak hindi .mr.modi prime minist tree give us oxygen take carbon di oxide.tre give us shelter fruit .',\n",
       " 'name school [ t.p. ] tagor public school , I studi class 4th , My hobbi danc , play basket ball badminton']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size=10000 # intializing dict size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[623, 6666, 2228, 5674, 7654, 2373, 7884, 6056, 6666, 504, 6137, 6780, 3368], [1216, 1846, 7970, 6209, 4067, 6691, 2207, 4143, 1216, 597, 1216, 1846, 6209, 1216, 1298, 2591, 4086, 7897, 4565, 7654, 8027, 4086, 5813], [4067, 7897, 5433, 3934, 4910, 699, 3851, 3011, 9445, 1846, 7970, 6514, 8336, 1483, 9140, 1630, 9793, 1846, 7970, 1828, 9124], [2228, 9427, 9583, 8758, 4368, 4849, 9427, 7654, 6137, 6780, 5412, 623, 9651, 7526, 2611, 9113, 9426, 5289]]\n"
     ]
    }
   ],
   "source": [
    "#performing one hot encoder after tokenizibg the paragragh to sentence->lemmatizer ->stop word\n",
    "onehot_repr=[one_hot(words,voc_size)for words in sentence5]\n",
    "print(onehot_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding # word embedding lib\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # padding lib (adding object)\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0  623 6666 2228 5674 7654 2373 7884 6056 6666  504 6137 6780\n",
      "  3368]\n",
      " [1216  597 1216 1846 6209 1216 1298 2591 4086 7897 4565 7654 8027 4086\n",
      "  5813]\n",
      " [3851 3011 9445 1846 7970 6514 8336 1483 9140 1630 9793 1846 7970 1828\n",
      "  9124]\n",
      " [8758 4368 4849 9427 7654 6137 6780 5412  623 9651 7526 2611 9113 9426\n",
      "  5289]]\n"
     ]
    }
   ],
   "source": [
    "sent_length=15 # creating max length of each sentence\n",
    "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
    "print(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=20 # dimension of feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Padam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Padam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,10,input_length=sent_length))\n",
    "model.compile('adam','mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 15, 10)            100000    \n",
      "=================================================================\n",
      "Total params: 100,000\n",
      "Trainable params: 100,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.0378104   0.03034979  0.02823548 -0.04777275 -0.01485823\n",
      "   -0.01449811  0.02384262  0.02593693 -0.00158877  0.03643635]\n",
      "  [ 0.0378104   0.03034979  0.02823548 -0.04777275 -0.01485823\n",
      "   -0.01449811  0.02384262  0.02593693 -0.00158877  0.03643635]\n",
      "  [ 0.04408422 -0.00552309 -0.04177277 -0.02362646 -0.03681508\n",
      "    0.01973139 -0.0389599   0.00637598  0.01944733 -0.00889196]\n",
      "  [-0.04107356  0.04162521  0.02460209 -0.0223356  -0.04982644\n",
      "   -0.0001495   0.04995283 -0.01908324  0.0300516  -0.0356784 ]\n",
      "  [ 0.01833004 -0.01696137 -0.01656103  0.01718411  0.00013441\n",
      "    0.02815253  0.03046114 -0.04540845 -0.01601128  0.00221536]\n",
      "  [-0.03875405  0.01628518  0.04612713 -0.00732879 -0.04206868\n",
      "   -0.04723082  0.01038979  0.03285423 -0.03102914  0.03978315]\n",
      "  [-0.01024108 -0.00437211 -0.04565406  0.03504002  0.01210215\n",
      "   -0.01624198  0.02502764 -0.03687607  0.04694596  0.01201053]\n",
      "  [-0.01282211 -0.04660086  0.00057878 -0.03679669  0.04575877\n",
      "   -0.0083198   0.01498369 -0.00087173 -0.03538816 -0.01039691]\n",
      "  [-0.03724686  0.00663631  0.04903636  0.00887067 -0.00220937\n",
      "   -0.02045908  0.0377002   0.00960628 -0.02378178 -0.01453142]\n",
      "  [ 0.01383405 -0.03320082  0.04817824  0.02294638  0.01041015\n",
      "    0.04803381  0.01268766 -0.02750974 -0.04034712 -0.04079089]\n",
      "  [-0.04107356  0.04162521  0.02460209 -0.0223356  -0.04982644\n",
      "   -0.0001495   0.04995283 -0.01908324  0.0300516  -0.0356784 ]\n",
      "  [-0.00245171 -0.03904127 -0.02529301 -0.01969911  0.04034941\n",
      "   -0.04041524 -0.02836248  0.00169463  0.01228083 -0.02543217]\n",
      "  [ 0.03487052 -0.01481521  0.02739339 -0.04814342  0.0188497\n",
      "    0.00695063 -0.00868658  0.01296363 -0.03585349  0.01873979]\n",
      "  [-0.03424935  0.00254507 -0.03322903 -0.04407943 -0.01763768\n",
      "   -0.02377793  0.00584909  0.02151835  0.04838855 -0.00903784]\n",
      "  [ 0.02284032 -0.0067737   0.03144702 -0.02616161  0.02134836\n",
      "   -0.02112305 -0.00711033  0.002792   -0.00663296 -0.04448651]]\n",
      "\n",
      " [[-0.02827437  0.03218916 -0.01860274 -0.02946085  0.01010394\n",
      "    0.02677425 -0.01410413  0.03062072  0.02470182 -0.0259761 ]\n",
      "  [ 0.00746328 -0.03408276  0.04964438  0.0304301   0.04457405\n",
      "    0.00713391 -0.03857871  0.02834425  0.00219349 -0.01147667]\n",
      "  [-0.02827437  0.03218916 -0.01860274 -0.02946085  0.01010394\n",
      "    0.02677425 -0.01410413  0.03062072  0.02470182 -0.0259761 ]\n",
      "  [ 0.03455261  0.02359625  0.03998109 -0.02551496  0.04321638\n",
      "    0.01481458 -0.0076135  -0.00802577 -0.01844059 -0.01455123]\n",
      "  [-0.04356062 -0.02292498 -0.02970208  0.01663459 -0.03722925\n",
      "   -0.02021892  0.01817012  0.04638794 -0.01857249 -0.04831984]\n",
      "  [-0.02827437  0.03218916 -0.01860274 -0.02946085  0.01010394\n",
      "    0.02677425 -0.01410413  0.03062072  0.02470182 -0.0259761 ]\n",
      "  [ 0.04972525  0.04740218  0.02358737 -0.04082577  0.0189423\n",
      "   -0.01274629  0.0471784  -0.04618108  0.03477914 -0.04688711]\n",
      "  [ 0.03677649  0.03547037  0.02414577  0.04505168 -0.00978056\n",
      "    0.01812187  0.03309718 -0.04826979  0.04373908  0.02295804]\n",
      "  [ 0.00954669  0.04412731 -0.02564052  0.04805788  0.02935591\n",
      "    0.02784647  0.01595679  0.00055861  0.02056079  0.0294858 ]\n",
      "  [ 0.02812011 -0.04427803 -0.0197953   0.03369715  0.04816965\n",
      "   -0.03253424 -0.0152092  -0.03008066 -0.04652598  0.02431456]\n",
      "  [ 0.03768467  0.04577434 -0.01680118 -0.01366442 -0.01041798\n",
      "    0.01093708  0.04091436 -0.01239411 -0.04925473  0.02635906]\n",
      "  [-0.01024108 -0.00437211 -0.04565406  0.03504002  0.01210215\n",
      "   -0.01624198  0.02502764 -0.03687607  0.04694596  0.01201053]\n",
      "  [ 0.01035992  0.0452009   0.04928384  0.04799843 -0.00738924\n",
      "    0.0233008   0.01638522  0.01077311  0.04911816 -0.01659096]\n",
      "  [ 0.00954669  0.04412731 -0.02564052  0.04805788  0.02935591\n",
      "    0.02784647  0.01595679  0.00055861  0.02056079  0.0294858 ]\n",
      "  [ 0.04547684  0.01563164  0.02412504 -0.00028282 -0.01454487\n",
      "    0.03166766 -0.02990234  0.01884092 -0.01771821  0.04541569]]\n",
      "\n",
      " [[ 0.04647347 -0.03525795  0.00752424  0.02559872  0.00332434\n",
      "    0.03707376  0.04623965 -0.04537474 -0.01569863 -0.00403476]\n",
      "  [-0.01916059  0.010185    0.03854318  0.03480358 -0.00488836\n",
      "    0.00833287 -0.00718693  0.04103507  0.01483909 -0.00145627]\n",
      "  [ 0.00349951 -0.00255943  0.00218207 -0.03159717 -0.02728846\n",
      "    0.00784544 -0.0407881   0.02933699  0.0150351   0.02162131]\n",
      "  [ 0.03455261  0.02359625  0.03998109 -0.02551496  0.04321638\n",
      "    0.01481458 -0.0076135  -0.00802577 -0.01844059 -0.01455123]\n",
      "  [-0.01094464  0.02098988 -0.04578282 -0.03853067 -0.04465288\n",
      "   -0.00716965  0.04885319  0.02606987  0.03285554  0.02491784]\n",
      "  [-0.04459789  0.04024613  0.04628653 -0.03032576  0.0394683\n",
      "    0.00142195  0.02568128 -0.03908615 -0.03227925 -0.02919384]\n",
      "  [ 0.00544507 -0.01492287  0.01256249 -0.01198038  0.01692117\n",
      "    0.03798706  0.04935359  0.03558833 -0.02065705 -0.0426026 ]\n",
      "  [ 0.00541792  0.01992277  0.0309226   0.00713588  0.0405317\n",
      "    0.02796623 -0.00777676 -0.00607117 -0.04894929 -0.0224584 ]\n",
      "  [-0.02984055 -0.01933112  0.02528682  0.01574527  0.00956694\n",
      "   -0.03406388  0.04567449 -0.03031222 -0.00446256 -0.01679366]\n",
      "  [ 0.01619122 -0.04127057 -0.0335192   0.04055723 -0.01603585\n",
      "    0.03539595  0.03633423  0.04233048 -0.04037225  0.01280579]\n",
      "  [-0.03136086  0.03609299  0.01093847  0.04247901  0.00623957\n",
      "    0.01449919 -0.01870389 -0.00303395  0.04189496 -0.00417049]\n",
      "  [ 0.03455261  0.02359625  0.03998109 -0.02551496  0.04321638\n",
      "    0.01481458 -0.0076135  -0.00802577 -0.01844059 -0.01455123]\n",
      "  [-0.01094464  0.02098988 -0.04578282 -0.03853067 -0.04465288\n",
      "   -0.00716965  0.04885319  0.02606987  0.03285554  0.02491784]\n",
      "  [ 0.02199553 -0.03064534  0.00258374  0.01777566  0.00540581\n",
      "    0.01653903 -0.01298118  0.01783993  0.001947    0.03053018]\n",
      "  [ 0.0437288   0.03825805 -0.0368472  -0.02145555  0.04636708\n",
      "   -0.03270215  0.0155807   0.03451454 -0.00210562 -0.01777135]]\n",
      "\n",
      " [[-0.03375405 -0.0419845   0.01939524 -0.04503898  0.04335849\n",
      "    0.03410268 -0.02569522  0.004339    0.01239951  0.02464646]\n",
      "  [-0.04937886  0.01872304  0.0381774   0.04114138  0.01565332\n",
      "   -0.00223086 -0.0400823   0.02441504  0.02718294  0.02896085]\n",
      "  [-0.03198015 -0.03085206 -0.04674936 -0.00250287  0.03924185\n",
      "   -0.04742432 -0.03949537  0.01534561  0.00515177 -0.01507036]\n",
      "  [ 0.03599873  0.02515038 -0.0285665  -0.01175023  0.01642409\n",
      "    0.04728236 -0.01782385  0.03925039 -0.01828699  0.03627427]\n",
      "  [-0.01024108 -0.00437211 -0.04565406  0.03504002  0.01210215\n",
      "   -0.01624198  0.02502764 -0.03687607  0.04694596  0.01201053]\n",
      "  [ 0.03487052 -0.01481521  0.02739339 -0.04814342  0.0188497\n",
      "    0.00695063 -0.00868658  0.01296363 -0.03585349  0.01873979]\n",
      "  [-0.03424935  0.00254507 -0.03322903 -0.04407943 -0.01763768\n",
      "   -0.02377793  0.00584909  0.02151835  0.04838855 -0.00903784]\n",
      "  [ 0.02010505 -0.02408873  0.04395269  0.04298473  0.00108882\n",
      "   -0.00651788  0.0443196   0.03701741  0.02078008  0.03145471]\n",
      "  [ 0.04408422 -0.00552309 -0.04177277 -0.02362646 -0.03681508\n",
      "    0.01973139 -0.0389599   0.00637598  0.01944733 -0.00889196]\n",
      "  [-0.01295769 -0.00189962  0.01726573  0.02620716  0.03907874\n",
      "    0.04925433  0.02271462  0.04437702 -0.0361989   0.01127006]\n",
      "  [ 0.01763773 -0.02355936 -0.02706544  0.00473462  0.01983309\n",
      "    0.03721944  0.03753895 -0.02887025 -0.00833151 -0.01219972]\n",
      "  [-0.0499948  -0.0278264  -0.00122057  0.04700551  0.04810934\n",
      "    0.00589328 -0.00769078 -0.00680177  0.01064544 -0.03830918]\n",
      "  [ 0.03739894 -0.01511768  0.00336932  0.03067942  0.04549526\n",
      "   -0.01381824  0.03629166  0.03297212  0.03507653 -0.04104636]\n",
      "  [-0.02000241  0.0078426   0.04923414  0.02927096  0.00495408\n",
      "   -0.00788603 -0.00326902 -0.02861211  0.04906055  0.02511508]\n",
      "  [-0.01302141  0.00108904  0.01601194 -0.01421969  0.04108466\n",
      "    0.01486604  0.04327751  0.03065537 -0.03558253  0.03480972]]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(embedded_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,  623, 6666, 2228, 5674, 7654, 2373, 7884, 6056, 6666,\n",
       "        504, 6137, 6780, 3368])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0378104   0.03034979  0.02823548 -0.04777275 -0.01485823 -0.01449811\n",
      "   0.02384262  0.02593693 -0.00158877  0.03643635]\n",
      " [ 0.0378104   0.03034979  0.02823548 -0.04777275 -0.01485823 -0.01449811\n",
      "   0.02384262  0.02593693 -0.00158877  0.03643635]\n",
      " [ 0.04408422 -0.00552309 -0.04177277 -0.02362646 -0.03681508  0.01973139\n",
      "  -0.0389599   0.00637598  0.01944733 -0.00889196]\n",
      " [-0.04107356  0.04162521  0.02460209 -0.0223356  -0.04982644 -0.0001495\n",
      "   0.04995283 -0.01908324  0.0300516  -0.0356784 ]\n",
      " [ 0.01833004 -0.01696137 -0.01656103  0.01718411  0.00013441  0.02815253\n",
      "   0.03046114 -0.04540845 -0.01601128  0.00221536]\n",
      " [-0.03875405  0.01628518  0.04612713 -0.00732879 -0.04206868 -0.04723082\n",
      "   0.01038979  0.03285423 -0.03102914  0.03978315]\n",
      " [-0.01024108 -0.00437211 -0.04565406  0.03504002  0.01210215 -0.01624198\n",
      "   0.02502764 -0.03687607  0.04694596  0.01201053]\n",
      " [-0.01282211 -0.04660086  0.00057878 -0.03679669  0.04575877 -0.0083198\n",
      "   0.01498369 -0.00087173 -0.03538816 -0.01039691]\n",
      " [-0.03724686  0.00663631  0.04903636  0.00887067 -0.00220937 -0.02045908\n",
      "   0.0377002   0.00960628 -0.02378178 -0.01453142]\n",
      " [ 0.01383405 -0.03320082  0.04817824  0.02294638  0.01041015  0.04803381\n",
      "   0.01268766 -0.02750974 -0.04034712 -0.04079089]\n",
      " [-0.04107356  0.04162521  0.02460209 -0.0223356  -0.04982644 -0.0001495\n",
      "   0.04995283 -0.01908324  0.0300516  -0.0356784 ]\n",
      " [-0.00245171 -0.03904127 -0.02529301 -0.01969911  0.04034941 -0.04041524\n",
      "  -0.02836248  0.00169463  0.01228083 -0.02543217]\n",
      " [ 0.03487052 -0.01481521  0.02739339 -0.04814342  0.0188497   0.00695063\n",
      "  -0.00868658  0.01296363 -0.03585349  0.01873979]\n",
      " [-0.03424935  0.00254507 -0.03322903 -0.04407943 -0.01763768 -0.02377793\n",
      "   0.00584909  0.02151835  0.04838855 -0.00903784]\n",
      " [ 0.02284032 -0.0067737   0.03144702 -0.02616161  0.02134836 -0.02112305\n",
      "  -0.00711033  0.002792   -0.00663296 -0.04448651]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(embedded_docs)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
